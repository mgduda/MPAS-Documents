\chapter{Building MPAS}
\label{chap:mpas_build_instructions}

\section{Prerequisites}
\label{build_prerequisites}

To build MPAS, compatible C and Fortran compilers are required. Additionally,
the MPAS software relies on the PIO parallel I/O library to read and write model
fields, and the PIO library requires the standard netCDF library as well as the
parallel-netCDF library from Argonne National Laboratory. All libraries must be
compiled with the same compilers that will be used to build MPAS. Section
\ref{sec:build_io} summarizes the basic procedure of installing the required I/O
libraries for MPAS.

In order for the MPAS makefiles to find the PIO, parallel-netCDF, and netCDF
include files and libraries, the environment variables {\tt PIO}, {\tt PNETCDF},
and {\tt NETCDF} should be set to the root installation directories of the PIO,
parallel-netCDF, and netCDF installations, respectively. 

An MPI installation such as MPICH or OpenMPI is also required, and there is no
option to build a serial version of the MPAS executables. MPAS-Atmosphere v5.0
introduces the capability to use hybrid parallelism using MPI and OpenMP; however,
the use of OpenMP {\em should be considered experimental} and generally does not
offer any performance advantage. The primary reason for releasing a shared-memory
capability is to make this code available to collaborators for future development.


\section{Compiling I/O Libraries}
\label{sec:build_io}

{\bf IMPORTANT NOTE:} {\em The instructions provided in this section for
installing libraries have been successfully used by MPAS developers, but due to
differences in library versions, compilers, and system configurations, it is
recommended that users consult documentation provided by individual library
vendors should problems arise during installation. The MPAS developers cannot assume
responsibility for third-party libraries.} \vspace{12pt}

Although most recent versions of the netCDF and parallel-netCDF libraries should work,
the most tested versions of these libraries are netCDF 4.4.x and parallel-netCDF 1.8.x.
Users are strongly encouraged to use either the latest PIO 2.x version from \url{https://github.com/NCAR/ParallelIO/},
or PIO versions 1.7.1 or 1.9.23, as other versions have not been tested or are known to not work with MPAS.
The netCDF and parallel-netCDF libraries must be installed before building the PIO library.

\subsection{NetCDF}
\label{serial_netcdf}

Version 4.4.x of the netCDF library may be downloaded from
\url{http://www.unidata.ucar.edu/downloads/netcdf/index.jsp}.
The Unidata page provides detailed instructions instructions for building the netCDF C and Fortran libraries;
both the C and Fortran interfaces are needed by PIO. If netCDF-4 support is desired, the zlib and HDF5 libraries
will need to be installed prior to building netCDF. {\em Before proceeding to compile PIO
the {\tt NETCDF} environment variable should be set to the netCDF root
installation directory.}

\subsection{Parallel-NetCDF}
\label{parallel_netcdf}

Version 1.8.x of the parallel-netCDF library may be downloaded from
\url{https://trac.mcs.anl.gov/projects/parallel-netcdf/wiki/Download}. {\em Before proceeding to
compile PIO the {\tt PNETCDF} environment variable should be set to the
parallel-netCDF root installation directory.}

\subsection{PIO}
\label{pio_build}

Beginning with the MPAS v5.2 release, the either of the PIO 1.x or 2.x library versions
may be used. The two major versions have slightly different APIs; by default, the MPAS build
system assumes the PIO 1.x API, but the PIO 2.x library versions may be used by
adding the {\tt USE\_PIO2=true} option when compiling MPAS as described in Section \ref{compiling_MPAS}.
If compiling with the PIO 1.x library versions, users are strongly encouraged to choose
either PIO 1.7.1 or PIO 1.9.23, as other 1.x versions may not work; these two specific versions may
be obtained from \url{https://github.com/NCAR/ParallelIO/releases/tag/pio1_7_1} and
\url{https://github.com/NCAR/ParallelIO/releases/tag/pio1_9_23}.

The PIO 2.x library versions support integrated performance timing with the GPTL library; however,
the MPAS infrastructure does not currently provide calls to initialize this library when it is used in PIO 2.x.
Therefore, it is recommended to add {\tt -DPIO\_ENABLE\_TIMING=OFF} when running the cmake command
to build PIO 2.x versions.

After PIO is built and installed the {\tt PIO} environment variable should be set to 
the directory where PIO was installed. Recent versions of PIO support the specification of an installation
prefix, while some older versions do not, in which case the {\tt PIO} environment variable should
be set to the directory where PIO was compiled.

\section{Compiling MPAS}
\label{compiling_MPAS}

{\bf IMPORTANT NOTE:} {\em Before compiling MPAS, the {\tt NETCDF}, {\tt
PNETCDF}, and {\tt PIO} environment variables must be set to the library
installation directories as described in the previous section.} \vspace{12pt}

The MPAS code uses only the `make' utility for compilation. Rather than
employing a separate configuration step before building the code, all
information about compilers, compiler flags, etc., is contained in the top-level
{\tt Makefile}; each supported combination of compilers (i.e., a configuration)
is included in the {\tt Makefile} as a separate make target, and the user
selects among these configurations by running {\tt make} with the name of a
build target specified on the command-line, e.g.,

\vspace{12pt}
{\tt > make gfortran}
\vspace{12pt}

\noindent to build the code using the GNU Fortran and C compilers. Some of the
available targets are listed in the table below, and additional targets can be
added by simply editing the {\tt Makefile} in the top-level directory.

\vspace{12pt}
\begin{longtable}{| l | l | l | l |}
\hline
Target & Fortran compiler & C compiler & MPI wrappers \\ \hline \hline
{\tt xlf} & xlf90 & xlc & mpxlf90 / mpcc \\ \hline
{\tt pgi} & pgf90 & pgcc & mpif90 / mpicc \\ \hline
{\tt ifort} & ifort & gcc & mpif90 / mpicc \\ \hline
{\tt gfortran} & gfortran & gcc & mpif90 / mpicc \\ \hline
{\tt llvm} & flang & clang & mpifort / mpicc \\ \hline
{\tt bluegene} & bgxlf95\_r & bgxlc\_r & mpxlf95\_r / mpxlc\_r \\ \hline
\end{longtable}
\vspace{12pt}

The MPAS framework supports multiple {\em cores} --- currently a shallow water
model, an ocean model, a land-ice model, a non-hydrostatic atmosphere model, and
a non-hydrostatic atmosphere initialization core --- so the build process must be told which core
to build. This is done by either setting the environment variable {\tt CORE} to
the name of the model core to build, or by specifying the core to be built
explicitly on the command-line when running {\tt make}. For the atmosphere
core, for example, one may run either

\vspace{12pt}
{\tt > setenv CORE atmosphere}

{\tt > make gfortran}
\vspace{12pt}

\noindent or

\vspace{12pt}
{\tt > make gfortran CORE=atmosphere}
\vspace{12pt}

If the {\tt CORE} environment variable is set and a core is specified on the
command-line, the command-line value takes precedence; if no core is specified,
either on the command line or via the {\tt CORE} environment variable, the build
process will stop with an error message stating such.  Assuming compilation is
successful, the model executable, named {\tt \$\{CORE\}\_model} (e.g., {\tt
atmosphere\_model}), should be created in the top-level MPAS directory.

In order to get a list of available cores, one can simply run the top-level {\tt
Makefile} without setting the {\tt CORE} environment variable or passing the
core via the command-line. An example of the output from this can be seen
below.

{\small
\begin{verbatim}
> make
( make error )
make[1]: Entering directory `/scratch/MPAS-Release'

Usage: make target CORE=[core] [options]

Example targets:
    ifort
    gfortran
    xlf
    pgi

Availabe Cores:
    atmosphere
    init_atmosphere
    landice
    ocean
    seaice
    sw
    test

Available Options:
    DEBUG=true    - builds debug version. Default is optimized version.
    USE_PAPI=true - builds version using PAPI for timers. Default is off.
    TAU=true      - builds version using TAU hooks for profiling. Default is off.
    AUTOCLEAN=true    - forces a clean of infrastructure prior to build new core.
    GEN_F90=true  - Generates intermediate .f90 files through CPP, and builds with them.
    TIMER_LIB=opt - Selects the timer library interface to be used for profiling the model. Options are:
                    TIMER_LIB=native - Uses native built-in timers in MPAS
                    TIMER_LIB=gptl - Uses gptl for the timer interface instead of the native interface
                    TIMER_LIB=tau - Uses TAU for the timer interface instead of the native interface
    OPENMP=true   - builds and links with OpenMP flags. Default is to not use OpenMP.
    USE_PIO2=true - links with the PIO 2 library. Default is to use the PIO 1.x library.
    PRECISION=single - builds with default single-precision real kind. Default is to use double-precision.

Ensure that NETCDF, PNETCDF, PIO, and PAPI (if USE_PAPI=true) are environment variables
that point to the absolute paths for the libraries.

************ ERROR ************
No CORE specified. Quitting.
************ ERROR ************
\end{verbatim}

\section{Selecting a single-precision build}
\label{sec:build_single_prec}

Beginning with version 2.0, MPAS-Atmosphere can be compiled and run in single-precision, offering faster 
model execution and smaller input and output files. Beginning with version 5.0, the selection of the
model precision can be made on the command-line, with no need to edit the {\tt Makefile}.
To compile a single-precision MPAS-Atmosphere executable, add {\tt PRECISION=single} to the build command, e.g.,

\vspace{12pt}
{\tt > make gfortran CORE=atmosphere PRECISION=single}
\vspace{12pt}

Regardless of which precision the MPAS-Atmosphere {\tt init\_atmosphere} and {\tt atmosphere} cores were compiled
with, either single- or double-precision input files may be used. In general, the MPAS infrastructure should
correctly detect the precision of input files, but one may also explicitly specify the precision of files in
an input stream by adding the {\tt precision} attribute to the stream definition as described in Section \ref{sec:optional_stream_atts}.

\section{Cleaning}

To remove all files  that were created when the model was built,
including the model executable itself, {\tt make} may be run for the
`clean' target:

\vspace{12pt}
{\tt > make clean}
\vspace{12pt}

As with compiling, the core to be cleaned is specified by the {\tt CORE}
environment variable, or by specifying a core explicitly on the
command-line with {\tt CORE=}.
